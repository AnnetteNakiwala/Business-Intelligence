{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnnetteNakiwala/Business-Intelligence/blob/main/Fraud_detection_on_online_bank_transactions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FRAUD DETECTION ON ONLINE BANK TRANSACTIONS**"
      ],
      "metadata": {
        "id": "dmn_TfmNsPAd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction:**\n",
        "\n",
        "The purpose of this project is to develop a fraud detection system for online transactions that uses machine learning models. The goal is to build a system that can accurately detect fraudulent transactions and prevent financial losses for both consumers and businesses.\n",
        "\n",
        "**Data:**\n",
        "\n",
        "The first step in this project is to collect and preprocess data from online transactions. This data should include information such as the transaction amount, the time and date of the transaction, the location of the transaction, and the type of payment method used. Other relevant data could include the user's location, device information, and behavior patterns.\n",
        "\n",
        "**Model Development:**\n",
        "\n",
        "The next step is to develop machine learning models that can accurately detect fraudulent transactions. This involves training the models on historical data that contains both fraudulent and legitimate transactions. The models should be able to learn patterns in the data that are associated with fraudulent activity and use those patterns to identify suspicious transactions.\n",
        "\n",
        "**Evaluation:**\n",
        "\n",
        "Once the models have been developed, they need to be evaluated to determine their effectiveness at detecting fraud. This involves testing the models on a separate set of data that contains both fraudulent and legitimate transactions. The performance of the models should be evaluated using metrics such as precision, recall, and F1 score.\n",
        "\n",
        "**Deployment:**\n",
        "\n",
        "Once the models have been developed and evaluated, they can be deployed in a real-world environment. This involves integrating the models into an online payment system and monitoring transactions in real-time. If a transaction is flagged as potentially fraudulent, it can be reviewed by a human analyst who can make the final determination about whether it is fraudulent or not.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "In conclusion, this project aims to develop a fraud detection system for online transactions that uses machine learning models. By accurately detecting fraudulent transactions, this system can prevent financial losses for both consumers and businesses and help to maintain the integrity of online payment systems.\n",
        "\n",
        "**Task Note:**\n",
        "1. Machine Leanring models to use:\n",
        "2. LogisticRegression\n",
        "3. KNeighborsClassifier\n",
        "4. RandomForestClassifier\n",
        "5. XGBClassifier\n",
        "6. SupportVectorMachine Classifier.\n",
        "\n",
        "**Data:**\n",
        "Download the dataset from this link below:\n",
        "https://www.kaggle.com/ntnu-testimon/paysim1\n",
        "\n",
        "**Dataset has fillowing columns:**\n",
        "\n",
        "**step** - maps a unit of time in the real world. In this case 1 step is 1 hour of time. Total steps 744 (30 days simulation).\n",
        "\n",
        "**type** - CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER.\n",
        "\n",
        "**amount** - amount of the transaction in local currency.\n",
        "\n",
        "**nameOrig** - customer who started the transaction\n",
        "\n",
        "**oldbalanceOrg** - initial balance before the transaction\n",
        "\n",
        "**newbalanceOrig** - new balance after the transaction\n",
        "\n",
        "**nameDest** - customer who is the recipient of the transaction\n",
        "\n",
        "**oldbalanceDest** - initial balance recipient before the transaction. Note that there is not information for customers that start with M (Merchants).\n",
        "\n",
        "**newbalanceDest** - new balance recipient after the transaction. Note that there is not information for customers that start with M (Merchants).\n",
        "\n",
        "**isFraud** - This is the transactions made by the fraudulent agents inside the simulation. In this specific dataset the fraudulent behavior of the agents aims to profit by taking control or customers accounts and try to empty the funds by transferring to another account and then cashing out of the system.\n",
        "\n",
        "**isFlaggedFraud** - The business model aims to control massive transfers from one account to another and flags illegal attempts. An illegal attempt in this dataset is an attempt to transfer more than 200.000 in a single transaction."
      ],
      "metadata": {
        "id": "EYkvMipzsUA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a5ian07UueIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FsWfAJN7uiIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploratory Data Analysis (EDA)**"
      ],
      "metadata": {
        "id": "dtFmYIw3upe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check the shape of the dataset\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iR7jpFdyulID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#examine the dataset using describe function\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7P0BihEVuzkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if there is anu null values\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "abbQgxc0u8BA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check for duplicate values\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m8bTAIi2vAMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distribution of all Transactions**"
      ],
      "metadata": {
        "id": "NlktUKfzvFbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Distribution of the frequency of all transactions \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZHh7z3givG9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#safe transactions amount distribution plot\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iFMJiIqKvNQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fraud transactions amount distribution plot\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gypCXkkWvdiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fraud transaction boxplot for amount distribution\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cRbkJOQIvjAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Type of Transactions**"
      ],
      "metadata": {
        "id": "dPVrpZZ3vqfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check the type of  safe transactions using count_values function\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3VWUNqc5vsEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the type of  fraud transactions using count_values function\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JOVNVgGWv3_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Machine learning**"
      ],
      "metadata": {
        "id": "L9UOdp6NwLMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#drop the name columns\n",
        "\n"
      ],
      "metadata": {
        "id": "Wpg5NLYQwOXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Binary-encoding of labelled data in 'type' for 'CASH_OUT' and 'TRANSFER' Columns\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_iOHdJffwjTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the target and features from the dataset\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZhlBGsz3wx8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into train and test\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8D2oXQoKw5qI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# General function to run classifier with default parameters to get baseline model\n",
        "def ml_func (algoritm):\n",
        "  #train and fit regression model\n",
        "\n",
        "  # predict\n",
        "  \n",
        "\n",
        "\n",
        "  # Evaluate\n",
        "\n",
        "\n",
        "\n",
        "  # store accuracy in a new dataframe\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p6K4Fp-vxE9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Machine Learning Algorithms Explained**\n",
        "\n",
        "**Logistic Regression:**\n",
        "\n",
        "Logistic Regression is a linear classification algorithm that is used to predict binary or multi-class outcomes. It uses the logistic function to estimate the probability of a binary response based on one or more predictor variables. It is commonly used in fields such as finance, marketing, and healthcare. In logistic regression, the coefficients are estimated using maximum likelihood estimation, and the model is evaluated based on metrics such as accuracy, precision, recall, and F1-score.\n",
        "\n",
        "**K-Nearest Neighbors Classifier:**\n",
        "\n",
        "The K-Nearest Neighbors (KNN) Classifier is a non-parametric algorithm that is used for classification and regression. KNN works by calculating the distances between a new observation and all the other observations in the dataset, and then selecting the K closest neighbors. The new observation is then classified based on the majority class of its K nearest neighbors. The value of K is chosen based on cross-validation and is evaluated based on metrics such as accuracy, precision, recall, and F1-score.\n",
        "\n",
        "**Random Forest Classifier:**\n",
        "\n",
        "Random Forest Classifier is an ensemble learning algorithm that is used for classification and regression. It works by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes of the individual trees. Random Forest Classifier is widely used because it is relatively easy to use, performs well on many datasets, and is resistant to overfitting. The performance of Random Forest Classifier is evaluated using metrics such as accuracy, precision, recall, and F1-score.\n",
        "\n",
        "**XGBClassifier:**\n",
        "\n",
        "XGBClassifier is an implementation of the Gradient Boosting algorithm that is designed for efficient and scalable tree boosting. Gradient Boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models. The XGBClassifier is widely used in Kaggle competitions and is known for its performance and speed. The performance of XGBClassifier is evaluated using metrics such as accuracy, precision, recall, and F1-score.\n",
        "\n",
        "**Support Vector Machine (SVM) Classifier:**\n",
        "\n",
        "Support Vector Machine (SVM) Classifier is a linear classification algorithm that is used for binary and multi-class classification. SVM works by finding a hyperplane that separates the data into different classes. SVM is commonly used in fields such as image recognition, text classification, and bioinformatics. The performance of SVM is evaluated using metrics such as accuracy, precision, recall, and F1-score. SVM can also be used for non-linear classification by using a kernel trick to transform the data into a higher dimensional space."
      ],
      "metadata": {
        "id": "71KnKp0CzMNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#list of all classifiers that I will run for base models \n",
        "algoritms=[LogisticRegression,KNeighborsClassifier,RandomForestClassifier,XGBClassifier,svm.SVC]\n",
        "\n",
        "#running each model and print accuracy scores\n",
        "for algoritm in algoritms:\n",
        "    ml_func (algoritm)"
      ],
      "metadata": {
        "id": "xiPdyzEpxhM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deployment of the most accourate model**\n",
        "\n",
        "Pickle is a Python module used for serializing and de-serializing Python object structures. It is used for data storage, data transfer, and object persistence. It can also be used to store trained machine learning models in Python, allowing you to save your model to disk and reload it at a later time.\n",
        "\n",
        "Here are the steps to use pickle to build a model in Python:\n",
        "\n",
        "1. Train your machine learning model using your dataset. For example, you might train a decision tree classifier to predict the class of an object based on its features.\n",
        "\n",
        "2. Once you have trained your model, import the pickle module and use the dump() method to save the trained model to disk. For example, you might save your decision tree classifier as follows:"
      ],
      "metadata": {
        "id": "-XCHCIaPyJNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# train decision tree classifier\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VX_kNOCvyUwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save model to using pickle\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GpTDnkw-yhaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model using pickle\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g03495xPyn02"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}